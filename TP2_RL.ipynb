{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9130815e-a2a7-4c2d-bb50-57bf5d61101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "obs , info = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985cdd0-c501-4bdb-8181-44dae1d8c236",
   "metadata": {},
   "source": [
    "Exploration d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6934ab-9f7d-48e6-98f5-2fb20ec804f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'observation: Discrete(16)\n",
      "Espace d'action: Discrete(4)\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 1 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 2 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 1 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 1 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 5, Récompense: 0.0, Terminé: True\n",
      "l'action : 1 , Observation: 1, Récompense: 0.0, Terminé: False\n",
      "l'action : 2 , Observation: 5, Récompense: 0.0, Terminé: True\n",
      "l'action : 3 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 1 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 3 , Observation: 1, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 5, Récompense: 0.0, Terminé: True\n",
      "l'action : 3 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 2 , Observation: 4, Récompense: 0.0, Terminé: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Espace d'observation:\", env.observation_space)\n",
    "print(\"Espace d'action:\", env.action_space)\n",
    "\n",
    "\n",
    "for _ in range(20):\n",
    "    action = env.action_space.sample() \n",
    "    obs, reward, done, _ , _ = env.step(action)  \n",
    "    \n",
    "    print(f\"l'action : {action} , Observation: {obs}, Récompense: {reward}, Terminé: {done}\")\n",
    "    \n",
    "    if done:\n",
    "        env.reset() \n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d7b0e-1282-4cd1-ac6f-d37ff1a41f50",
   "metadata": {},
   "source": [
    "Création de Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ec8a8f-9e6a-4bff-a75e-e3f9450c7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "états = env.observation_space.n  \n",
    "actions = env.action_space.n  \n",
    "\n",
    "q_table = np.zeros((états,actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfca318-802f-4498-b1fe-cde743b1e8c7",
   "metadata": {},
   "source": [
    "Affichage du tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1591abe1-0b48-4afd-92bb-f902e14fe30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a6aace-8257-4da4-849a-33098bcac6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "État 0: [0. 0. 0. 0.]\n",
      "État 1: [0. 0. 0. 0.]\n",
      "État 2: [0. 0. 0. 0.]\n",
      "État 3: [0. 0. 0. 0.]\n",
      "État 4: [0. 0. 0. 0.]\n",
      "État 5: [0. 0. 0. 0.]\n",
      "État 6: [0. 0. 0. 0.]\n",
      "État 7: [0. 0. 0. 0.]\n",
      "État 8: [0. 0. 0. 0.]\n",
      "État 9: [0. 0. 0. 0.]\n",
      "État 10: [0. 0. 0. 0.]\n",
      "État 11: [0. 0. 0. 0.]\n",
      "État 12: [0. 0. 0. 0.]\n",
      "État 13: [0. 0. 0. 0.]\n",
      "État 14: [0. 0. 0. 0.]\n",
      "État 15: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for etat in range(états):\n",
    "    print(f\"État {etat}: {q_table[etat]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0b3a3-db12-4d82-818a-c6952c9b9204",
   "metadata": {},
   "source": [
    "Implémentation d'algorithme Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11ffa26b-76ad-4d8c-855e-ef0b3949c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 0, Exploration : 0.9950, Récompense totale : 0.0\n",
      "Épisode 100, Exploration : 0.6027, Récompense totale : 0.0\n",
      "Épisode 200, Exploration : 0.3651, Récompense totale : 0.0\n",
      "Épisode 300, Exploration : 0.2212, Récompense totale : 1.0\n",
      "Épisode 400, Exploration : 0.1340, Récompense totale : 0.0\n",
      "Épisode 500, Exploration : 0.0812, Récompense totale : 0.0\n",
      "Épisode 600, Exploration : 0.0492, Récompense totale : 1.0\n",
      "Épisode 700, Exploration : 0.0298, Récompense totale : 1.0\n",
      "Épisode 800, Exploration : 0.0180, Récompense totale : 1.0\n",
      "Épisode 900, Exploration : 0.0109, Récompense totale : 1.0\n",
      "Épisode 1000, Exploration : 0.0066, Récompense totale : 1.0\n",
      "Épisode 1100, Exploration : 0.0040, Récompense totale : 1.0\n",
      "Épisode 1200, Exploration : 0.0024, Récompense totale : 1.0\n",
      "Épisode 1300, Exploration : 0.0015, Récompense totale : 1.0\n",
      "Épisode 1400, Exploration : 0.0009, Récompense totale : 1.0\n",
      "Épisode 1500, Exploration : 0.0005, Récompense totale : 1.0\n",
      "Épisode 1600, Exploration : 0.0003, Récompense totale : 1.0\n",
      "Épisode 1700, Exploration : 0.0002, Récompense totale : 1.0\n",
      "Épisode 1800, Exploration : 0.0001, Récompense totale : 1.0\n",
      "Épisode 1900, Exploration : 0.0001, Récompense totale : 1.0\n",
      "Épisode 2000, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2100, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2200, Exploration : 0.0000, Récompense totale : 0.0\n",
      "Épisode 2300, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2400, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2500, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2600, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 2700, Exploration : 0.0000, Récompense totale : 0.0\n",
      "Épisode 2800, Exploration : 0.0000, Récompense totale : 0.0\n",
      "Épisode 2900, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3000, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3100, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3200, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3300, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3400, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3500, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3600, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 3700, Exploration : 0.0000, Récompense totale : 0.0\n",
      "Épisode 3800, Exploration : 0.0000, Récompense totale : 0.0\n",
      "Épisode 3900, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4000, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4100, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4200, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4300, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4400, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4500, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4600, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4700, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4800, Exploration : 0.0000, Récompense totale : 1.0\n",
      "Épisode 4900, Exploration : 0.0000, Récompense totale : 1.0\n",
      "\n",
      " q_Table entraînée :\n",
      "[[0.52513679 0.45445791 0.45851161 0.45611292]\n",
      " [0.26244821 0.242494   0.17882762 0.45709446]\n",
      " [0.39435007 0.29121617 0.24058996 0.291813  ]\n",
      " [0.0633876  0.17867216 0.0861695  0.1645967 ]\n",
      " [0.56726955 0.43671151 0.36519585 0.22499382]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28687999 0.12626176 0.1266728  0.04396502]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47078703 0.28783355 0.38092859 0.61294795]\n",
      " [0.34772283 0.64081233 0.37245888 0.29525395]\n",
      " [0.58281357 0.37127378 0.31069523 0.34476385]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.48124936 0.52527169 0.73079936 0.42656057]\n",
      " [0.67579246 0.88114083 0.70649926 0.70423054]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1        # Taux d'apprentissage\n",
    "gamma = 0.99       # Facteur de discount\n",
    "epsilon = 1.0      # Exploration initiale\n",
    "epsilon_decay = 0.995  # Réduction d'exploration\n",
    "num_episodes = 100  # Nombre total d'épisodes\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs,info = env.reset()  \n",
    "    done = False  \n",
    "    rewards = 0 \n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.rand() < epsilon:  \n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(q_table[obs, :])  \n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        q_table[obs, action] = q_table[obs, action] + alpha * ( reward + gamma * np.max(q_table[next_obs, :]) - q_table[obs, action] )\n",
    "\n",
    "        obs = next_obs  \n",
    "        rewards += reward  \n",
    "\n",
    "   \n",
    "    epsilon *= epsilon_decay  \n",
    "   \n",
    "\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Épisode {episode}, Exploration : {epsilon:.4f}, Récompense totale : {rewards}\")\n",
    "\n",
    "\n",
    "print(\"\\n q_Table entraînée :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a7937-9d7a-4924-89c0-a470b29f5c71",
   "metadata": {},
   "source": [
    "Evaluation des Performances d'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58532f75-a7e0-47f7-955c-8e06f908d254",
   "metadata": {},
   "source": [
    "utilisant 5 comme nombre maximal de pas pour commencer une autre episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38b493ac-97b7-4a2c-9689-bd2587643d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 8, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 8, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 0, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 4, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 8, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 8, Récompense: 0.0, Terminé: False\n",
      "l'action : 0 , Observation: 12, Récompense: 0.0, Terminé: True\n",
      "la moyenne de réussit est 0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "successes = 0\n",
    "moyenne=0\n",
    "for episode in range(episodes):\n",
    "    num_action = 0 \n",
    "    env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for j in range(5)  : \n",
    "          action = np.argmax(q_table[obs, :])  \n",
    "          obs , reward , done , _ , _ = env.step(action)\n",
    "          print(f\"l'action : {action} , Observation: {obs}, Récompense: {reward}, Terminé: {done}\")\n",
    "          successes += reward \n",
    "    if done:  \n",
    "         if successes == 1 :\n",
    "            num_action+=1 \n",
    "         break \n",
    "    moyenne = successes / episodes          \n",
    "print(f\"la moyenne de réussit est {moyenne}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542908c5-0116-4b58-b0db-142b00911c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67e4cf-db7b-46d7-b70a-284844de5f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
